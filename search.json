[{"date":"2023-03-23T05:48:30.842Z","url":"/collaboratory-for-big-data/2023/03/23/%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","categories":[["undefined",""]],"content":"分类损失函数分类交叉熵分类交叉熵（categorical cross-entropy）是一种用于多分类问题的损失函数。它的计算基于模型的预测概率分布和真实标签的one-hot向量，其中每个元素表示样本属于对应类别的概率，只有一个元素为1，其余元素为0。假设有n个样本，每个样本有k个可能的类别，则模型的预测结果可以表示为一个形状为(n, k)的概率矩阵，真实标签可以表示为一个形状为(n, k)的one-hot矩阵。 分类交叉熵的计算公式如下： $ J(\\theta) &#x3D; -\\frac{1}{n} \\sum_{i&#x3D;1}^n \\sum_{j&#x3D;1}^k y_{ij} \\log(p_{ij}) $ 其中，$y_{ij}$是样本i属于类别j的one-hot标签，$p_{ij}$是模型预测样本i属于类别j的概率。公式中的第一个求和符号遍历所有样本，第二个求和符号遍历所有类别。 分类交叉熵的含义是用来评估模型预测结果与真实标签之间的差异。当模型预测结果和真实标签一致时，交叉熵为0；当它们之间的差距越大时，交叉熵的值越大。因此，分类交叉熵可以作为模型的损失函数，用来优化模型的参数。 在实际训练中，分类交叉熵通常与反向传播算法结合使用，以计算每个参数的梯度并更新参数。可以使用基于梯度下降的优化算法，如随机梯度下降（SGD）或Adam来最小化分类交叉熵损失，以提高模型的分类精度。 稀疏分类交叉熵稀疏分类交叉熵和普通的分类交叉熵在计算损失函数的方式上有所不同。 在普通的分类交叉熵中，目标值（即真实标签）采用了one-hot编码的方式，即只有真实类别对应的那个元素为1，其余元素都为0。在计算损失函数时，需要将神经网络输出的概率分布和目标值进行对比，计算交叉熵损失。 而在稀疏分类交叉熵中，目标值采用的是整数形式的标签，即每个样本的真实标签对应一个整数值。这种方式避免了使用one-hot编码所产生的大量零元素，节省了存储空间。在计算损失函数时，需要将神经网络输出的概率分布和目标值进行对比，计算交叉熵损失。 因此，稀疏分类交叉熵和普通的分类交叉熵的主要区别在于目标值的表示方式不同，稀疏分类交叉熵适合处理类别数目较多的情况，能够更有效地处理稀疏数据。 假设有一个图像分类问题，共有10个类别。使用稠密编码（dense encoding），每个样本的标签是一个长度为10的向量，每个元素代表一个类别，值为1表示属于该类别，值为0表示不属于。比如样本1属于第5个类别，则其标签为[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]。 使用稀疏编码（sparse encoding），每个样本的标签是一个标量，表示该样本所属的类别。比如样本1属于第5个类别，则其标签为4（从0开始计数）。在训练模型时，使用稀疏编码可以节省存储空间和计算资源。 对于稠密编码，其分类交叉熵的计算方式为： $-\\sum_{i&#x3D;1}^{10} y_i \\log(\\hat{y_i})$ 其中，$y_i$是样本的真实标签中第$i$个元素的值，$\\hat{y_i}$是模型预测的该元素的概率值。 对于稀疏编码，其分类交叉熵的计算方式为： $-\\log(\\hat{y_j})$ 其中，$j$是样本的真实标签。由于真实标签只有一个元素为1，因此只需要计算该元素对应的预测概率值的对数即可。 二元熵Binary Cross-entropy（二元交叉熵）是一种衡量两个概率分布相似性的方法，常用于二分类问题中。在机器学习中，它通常被用来衡量预测值与实际值之间的差异。 举个例子，假设我们有一个二分类问题，数据标签为0或1。我们的模型输出的预测值为y_pred，实际值为y_true。Binary Cross-entropy可以通过以下公式计算： $H(p,q)&#x3D;-\\frac{1}{N}\\sum_{i&#x3D;1}^{N}[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)]$ 其中，p表示模型输出的概率分布，q表示实际的标签分布。N表示样本数量，yi表示第i个样本的标签。 Binary Cross-entropy可以解释为模型对于一个样本预测为正类的不确定性，即模型预测正类的概率是多少。当预测越接近实际标签时，交叉熵越小，当预测与实际标签不一致时，交叉熵越大。 这个公式也可以扩展到多分类问题中，称为Categorical Cross-entropy。它的计算方法类似，不同之处在于标签使用独热编码。"},{"date":"2023-03-23T05:48:30.839Z","url":"/collaboratory-for-big-data/2023/03/23/adam%EF%BC%88adaptive%20moment%20estimation%EF%BC%89/","categories":[["undefined",""]],"content":"介绍Adam（Adaptive Moment Estimation）算法是一种优化算法，用于梯度下降过程中的参数更新，特别是在深度学习模型中广泛使用。 Adam算法是基于梯度下降算法的变体，结合了动量梯度下降和RMSProp算法的优点，旨在通过自适应调整学习率和动量，加速训练并获得更好的结果。 具体而言，Adam算法通过计算每个参数的指数移动平均值和平方梯度的指数移动平均值来更新参数。它包含以下步骤： 初始化参数：设置学习率、动量参数和平方梯度的指数移动平均参数。 计算梯度：计算当前参数的梯度。 计算动量：计算动量向量，考虑当前梯度和上一次动量向量。 计算平方梯度的指数移动平均值：计算当前参数的平方梯度，使用指数移动平均来计算平方梯度的指数移动平均值。 更新参数：使用动量和平方梯度的指数移动平均值来更新参数。 重复步骤2-5，直到达到最大迭代次数或收敛。 Adam算法有许多超参数可以调整，如学习率、动量参数和平方梯度的指数移动平均参数等。通常情况下，可以使用默认值或根据经验进行调整。 Adam算法在深度学习中表现出色，具有较快的收敛速度和较好的泛化性能。然而，它并不适用于所有类型的问题，可能需要根据具体问题进行优化算法选择。 详解RMSProp（Root Mean Square Propagation）算法和动量梯度下降（Momentum Gradient Descent）是两种常用的梯度下降算法，它们分别在Adam算法中被用到。 RMSProp算法 RMSProp算法主要用于解决梯度下降算法中学习率的问题，它的核心思想是自适应地调整每个参数的学习率。具体而言，RMSProp算法计算每个参数的平方梯度的指数移动平均值，然后将学习率除以该指数移动平均值的平方根，从而使每个参数的学习率根据其梯度的大小进行自适应调整。(参数移动越大，学习率越小) RMSProp算法可以避免学习率在训练期间过于波动，以及过度更新导致的问题，使得梯度下降算法更加稳定。 动量梯度下降 动量梯度下降算法是一种优化算法，旨在加速模型的训练过程，并且可以帮助算法避免陷入局部最优解。动量梯度下降算法通过引入动量（momentum）的概念，来模拟物理学中物体的惯性，使得梯度下降算法可以更快地收敛，并减少训练期间的震荡。 具体而言，动量梯度下降算法将梯度的更新分为两个部分：一个是当前梯度的贡献，另一个是历史梯度的贡献，历史梯度的贡献是通过引入动量参数来实现的，该参数表示历史梯度在更新中的权重。 动量梯度下降算法可以帮助算法跳出局部最优解，同时也可以减少梯度下降算法在平坦区域的震荡，从而加速模型训练过程。 总之，RMSProp算法和动量梯度下降算法都是常用的优化算法，它们可以帮助算法在训练期间更加稳定和快速地收敛。在深度学习中，这两个算法通常被用于Adam算法中，以提高算法的效率和性能。 缺点Adam算法在深度学习中表现优异，但并不适用于所有类型的问题。以下是一些可能不适合使用Adam算法的情况： 数据集较小：当训练数据集很小或具有噪声时，Adam算法可能会在噪声中迷失方向，导致模型性能下降。 学习率较高：当学习率设置过高时，Adam算法可能会导致参数更新过于频繁，使得模型在训练期间发生震荡和不稳定。 非凸优化问题：在非凸优化问题中，Adam算法可能会在局部最小值处陷入困境，而无法找到全局最小值。 对于这些问题，可以考虑使用其他优化算法来解决，例如： 数据集较小：可以使用随机梯度下降（Stochastic Gradient Descent，SGD）算法，通过在每个批次上进行梯度下降来避免过拟合。 学习率较高：可以使用Adagrad算法，它可以自适应地调整每个参数的学习率，避免过度更新。 非凸优化问题：可以使用L-BFGS算法等二阶优化算法，以快速找到全局最小值。 总之，选择优化算法应该考虑具体问题和数据集的特征，并根据实验结果进行调整和优化。"},{"title":"Hello World","date":"2023-03-23T01:21:28.372Z","url":"/collaboratory-for-big-data/2023/03/23/hello-world/","categories":[["undefined",""]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post More info: Writing Run server More info: Server Generate static files More info: Generating Deploy to remote sites More info: Deployment"}]